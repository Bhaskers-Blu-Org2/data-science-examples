{"cells":[{"cell_type":"markdown","source":["## Parameterized Job for running encryption"],"metadata":{}},{"cell_type":"markdown","source":["This notebook is a template to illustrate how encryption at scale can be parameterized so that it can be run with  data factory scheduling or Databricks job scheduling. The main use is to read json param file that contain mapping between dataset name and its schema definition & encryption definition so that we can automate: \n1. Reading datasets of different source types: flat file (csv and other), streaming from eventhubs and decode from raw into structured format\n2. Automatically pick up fields to encrypt\n3. Write to a designated delta table\nNote:\n- The structurization of data sources is currently done in a simplistic way, e.g. support string, numeric and date column. For other complex types like timestamp, reformat of date etc...future effort is needed to enhance. The main focus of this is to get the structure out in string and apply encryption\n- For parallel running of multiple notebook like this where each notebook process one dataset/table, one can either define parallel execution steps in ADF or use databricks workflow to run notebooks conrrently (https://docs.databricks.com/user-guide/notebooks/notebook-workflows.html#run-multiple-notebooks-concurrently). In that case, the step to load json file should be placed at the master notebook and this notebook is used as worker notebook to run a specific dataset\n\nParameters for the job/notebook\nThese are parameters for the notebook that can be set at run time either by Devop engineer or by another master notebook/job\n1. schema_mapping_json_path: Path to the Json file that contain mapping between dataset names, fields, whether or not it needs encryption, data type. See below for an example of a json file\n2. Dataset name: name of the dataset that will be used to look up for schema and encryption mapping in the json file\n3. Ingestion type: Streaming or Batch. Streaming is used when the source is EventHub. Batch is used when raw data is copied to a landing folder in ADLS Gen 2 and this notebook picks up from there\n4. Input data folder: For batch processing from landing zone. This can be set with wildcat to copy data recursively\n5. Output data folder: In case you let the job create the delta table automatically in batch mode, set the output folder so that encrypted data is created there. For batch mode, this may be needed as there can be multiple checkpoint location before final append to the target table. In case of streaming, the final table can be used directly as Streaming supports checkpoint location\n6. Table name: output table name. Can be an existing table name\n7. Eventhub Account: the account name of the eventhub in case a streaming dataset is used\n8. Secret scope/secret: name of secret scope and key to retrieve EH's key \n9. EH topic: topic to read data from. As a deviation from original design due to the use of Delta table, it's recommended to write streaming encryption output to a delta table instead of EH's topic. The reason is EH delta table support change capture. So the next job can just subscribe to a Delta table to read new changes. This may be faster than reading from a EH's topic."],"metadata":{}},{"cell_type":"markdown","source":["{\n  \"datasets\": {\n    \"chicago_crimes\": {\n      \"first_row_is_header\": \"true\",\n      \"field_target_dtype_mapping\": {\n        \"Seq\": \"IntegerType()\",\n        \"ID\": \"IntegerType()\",\n        \"Case_Number\": \"StringType()\",\n        \"Date\": \"StringType()\",\n        \"Block\": \"StringType()\",\n        \"IUCR\": \"StringType()\",\n        \"Primary_Type\": \"StringType()\",\n        \"Description\": \"StringType()\",\n        \"Location_Description\": \"StringType()\",\n        \"Beat\": \"StringType()\",\n        \"Arrest\": \"StringType()\",\n        \"Domestic\": \"StringType()\",\n        \"District\": \"StringType()\",\n        \"Ward\": \"StringType()\",\n        \"Community_Area\": \"StringType()\",\n        \"FBI_Code\": \"StringType()\",\n        \"X_Coordinate\": \"StringType()\",\n        \"Y_Coordinate\": \"StringType()\",\n        \"Year\": \"StringType()\",\n        \"Updated_On\": \"StringType()\",\n        \"Latitude\": \"StringType()\",\n        \"Longitude\": \"StringType()\",\n        \"Location\": \"StringType()\"\n      },\n      \"field_encryption_mapping\": {\n        \"Description\": \"alpha\",\n        \"Location_Description\": \"alpha\",\n        \"Block\": \"alpha\",\n        \"X_Coordinate\": \"alpha\",\n        \"Y_Coordinate\": \"alpha\"\n      },\n      \"delimiter\": \",\",\n      \"format\": \"csv\"\n    },\n    \"Event_json_flat\": {\n      \"field_target_dtype_mapping\": {\n        \"time\": \"TimestampType()\",\n        \"action\": \"StringType()\"\n      },\n      \"field_encryption_mapping\": {\n        \"time\": \"alpha\",\n        \"action\": \"alpha\"\n      },\n      \"format\": \"json\"\n    },\n    \"Event_json_stream\": {\n      \"field_target_dtype_mapping\": {\n        \"time\": \"TimestampType()\",\n        \"action\": \"StringType()\"\n      },\n      \"field_encryption_mapping\": {\n        \"time\": \"alpha\",\n        \"action\": \"alpha\"\n      },\n      \"format\": \"json\"\n    }\n  }\n}"],"metadata":{}},{"cell_type":"code","source":["//uncomment and Run this here or set it at the cluster level if you'd like to access ADLS Gen2 store directly instead of using mount point. Currently mount point of ADLS Gen 2 is not supported for Delta table\n// spark.conf.set(\"fs.azure.account.auth.type..dfs.core.windows.net\", \"OAuth\")\n// spark.conf.set(\"fs.azure.account.oauth.provider.type.adlsdatalakegen6.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n// spark.conf.set(\"fs.azure.account.oauth2.client.id..dfs.core.windows.net\", \"\")\n// spark.conf.set(\"fs.azure.account.oauth2.client.secret..dfs.core.windows.net\", \"\")\n// spark.conf.set(\"fs.azure.account.oauth2.client.endpoint..dfs.core.windows.net\", \"\")\n// spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\n// dbutils.fs.ls(\"abfss://test@.dfs.core.windows.net/\")\n// spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sql drop table test_db.my_table5 "],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%sql \n---replace this with the name of the table you'll use if you want to test the job with existing table instead of letting the job to create the table itself\n-- CREATE TABLE  iF NOT EXISTS test_db.my_table5 (action STRING, time string)\n-- USING DELTA LOCATION 'abfss://test@adlsdatalakegen6.dfs.core.windows.net/testencrypt2'"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%python\n# dbutils.widgets.removeAll()\n\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%python\nfrom pyspark.sql.types import *\nimport json\nfrom json import JSONDecoder\nfrom collections import OrderedDict\n\ndbutils.widgets.dropdown(\"ingestion_type\", \"Batch\", [\"Batch\", \"Streaming\"], \"Ingestion type\")\ndbutils.widgets.text(\"dataset_name\", \"chicago_crimes\", \"Dataset name\")\ndbutils.widgets.text(\"output_tbl\", \"output_table\", \"output delta table name \")\n\ndbutils.widgets.text(\"eh_topic\", \"kafka_spark\", \"Input eventHub topic\")\ndbutils.widgets.text(\"secret_scope\", \"encryptionkey\", \"Secret scope\")\ndbutils.widgets.text(\"secret_key\", \"evh\", \"Secret key\")\n\ndbutils.widgets.text(\"checkpointLocation\", \"dbfs:/mnt/cp/testcp\", \"Check point Location for streaming\")\n\ndbutils.widgets.text(\"input_path\", \"/FileStore/tables/Chicago_*.csv\", \"Input Data Folder\")\ndbutils.widgets.text(\"eh_account\", \"kafkaeventhub01\", \"Event Hub Account\")\ndbutils.widgets.text(\"en_schema_path\", \"/dbfs/FileStore/tables/enc_schema.json\", \"Json Encryption Schema Path\")\ndbutils.widgets.text(\"output_path\", \"/FileStore/output/output.delta\", \"Output File Path\")\n\ndataset_name = dbutils.widgets.get(\"dataset_name\")\n\ningestion_type = dbutils.widgets.get(\"ingestion_type\")\nen_schema_path =dbutils.widgets.get(\"en_schema_path\")\ninput_path = dbutils.widgets.get(\"input_path\")\ncheckpointLocation = dbutils.widgets.get(\"checkpointLocation\")\n\neh_account = dbutils.widgets.get(\"eh_account\")\noutput_path=dbutils.widgets.get(\"output_path\")\noutput_tbl=dbutils.widgets.get(\"output_tbl\")\n\neh_topic=dbutils.widgets.get(\"eh_topic\")\nsecret_scope=dbutils.widgets.get(\"secret_scope\")\nsecret_key=dbutils.widgets.get(\"secret_key\")\n\n\n#Need a method to maintain order of json fields in the mapping\njson_data=open(en_schema_path).read()\ncustomdecoder = JSONDecoder(object_pairs_hook=OrderedDict)\njsondata = customdecoder.decode(json_data)\n\nfield_encryption_mapping =jsondata.get('datasets').get(dataset_name).get('field_encryption_mapping')\ndata_format = jsondata.get('datasets').get(dataset_name).get(\"format\")\ndelimiter= jsondata.get('datasets').get(dataset_name).get(\"delimiter\")\nschema_text = jsondata.get('datasets').get(dataset_name).get('field_target_dtype_mapping')\nschema = StructType()\n\nfor (field,dtype) in schema_text.items():\n  if dtype =='StringType()':\n    dtype =StringType()\n  elif dtype =='IntegerType()':\n    dtype =IntegerType()\n  elif dtype =='LongType()':\n    dtype =LongType()\n  elif dtype =='LongType()':\n    dtype =LongType()\n  elif dtype =='DateType()':\n    dtype =DateType()\n  elif dtype =='TimestampType()':\n    dtype =TimestampType()\n  schema.add(field, data_type = dtype)\n\nprint(ingestion_type)\nprint(en_schema_path)\nprint(data_format)\nprint(input_path)\nprint(eh_account)\nprint(output_path)\nprint(schema)\nprint(eh_topic)\nprint(secret_scope)\nprint(secret_key)\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%python\n#below is to clear the path before each run, need to disable this for real run\nfrom pyspark.sql.functions import *\n\ndbutils.fs.rm(output_path, True)\ndbutils.fs.rm(checkpointLocation, True)\nspark.sql(\"drop table if exists \"+output_tbl)\n\n#Batch copy and encryption\ntemp_tbl_name = 'temp_tbl'\nencrypt_func='encrypt'\nif ingestion_type == \"Batch\":\n  if data_format =='csv':\n    first_row_is_header = \"true\"\n    df = spark.read.format(data_format) \\\n    .option(\"header\", first_row_is_header) \\\n    .option(\"sep\", delimiter) \\\n    .schema(schema) \\\n    .load(input_path) \n  else:\n    df = spark.read.format(data_format) \\\n    .schema(schema) \\\n    .load(input_path)     \nelse: #Prepare for loading eventHub info\n  eh_key = dbutils.secrets.get(secret_scope,secret_key)\n  BOOTSTRAP_SERVERS = eh_account+\".servicebus.windows.net:9093\"\n  EH_SASL = \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='$ConnectionString' password='Endpoint=sb://\"+eh_account+\".servicebus.windows.net;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=\"+eh_key+\"';\"\n  GROUP_ID = \"$Default\"\n  df = spark \\\n  .readStream \\\n  .format(\"kafka\") \\\n  .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS) \\\n  .option(\"subscribe\", eh_topic) \\\n  .option(\"kafka.sasl.mechanism\",\"PLAIN\") \\\n  .option(\"kafka.security.protocol\",\"SASL_SSL\") \\\n  .option(\"kafka.sasl.jaas.config\", EH_SASL ) \\\n  .option(\"kafka.request.timeout.ms\", \"60000\") \\\n  .option(\"kafka.session.timeout.ms\", \"60000\") \\\n  .option(\"kafka.group.id\", GROUP_ID) \\\n  .option(\"failOnDataLoss\", \"false\") \\\n  .load() \\\n  .select(col(\"timestamp\"), col(\"value\").cast(\"STRING\").alias(\"value\")) \\\n  .select(col(\"timestamp\"), from_json(col(\"value\"), schema).alias(\"json\"))\n  \n\ndf.registerTempTable(temp_tbl_name)\n\nsql_statement = \"Select \"\n#Where statement to remove null value object\nwhere_statement = \" where \"\nfor field in schema_text.keys():\n  if field in field_encryption_mapping.keys():\n    #in case of streaming above, the field need to be accessed as json.field_name\n    if ingestion_type == \"Batch\":\n      sql_statement = sql_statement  +encrypt_func +\"(\"+field+\",'\"+ field_encryption_mapping.get(field)+\"')  \"+field+\" ,\" \n      where_statement = where_statement +field+ \" is not null and \"\n    else:\n      sql_statement = sql_statement  +encrypt_func +\"(json.\"+field+\",'\"+ field_encryption_mapping.get(field)+\"')  \"+field+\" ,\" \n      where_statement = where_statement +\"json.\"+field+ \" is not null and \"\n      \n  else:\n    if ingestion_type == \"Batch\":\n      sql_statement = sql_statement+ field+\",\" \n    else:\n      sql_statement = sql_statement+\"json.\"+ field+\",\"\nsql_statement = sql_statement[:-1] + \" from \" + temp_tbl_name +  where_statement[:-4]\nprint(sql_statement)\ndf = spark.sql(sql_statement)\n\n#Write out the encrypted dataset\nif ingestion_type == \"Batch\":\n  df.write.format(\"delta\").saveAsTable(output_tbl, path=output_path)\nelse:#write stream to the delta table\n  df.writeStream \\\n  .format(\"delta\") \\\n  .outputMode(\"append\") \\\n  .option(\"checkpointLocation\", checkpointLocation) \\\n  .table(\"output_tbl\") \n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["//Show how the next job can just subscribe to delta table to read new data automatically without having to read from EH.\n// display(spark.readStream.table(\"test_db.my_table5\"))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%python\n#To simulate a stream to Kakfa for testing purpose, uncomment and run the following \n# from pyspark.sql.types import *\n# from pyspark.sql.functions import *\n\n# inputPath = \"/databricks-datasets/structured-streaming/events/\"\n\n# # Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\n# jsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n\n\n# # Similar to definition of staticInputDF above, just using `readStream` instead of `read`\n# streamingInputDF = (\n#   spark\n#     .readStream                       \n#     .schema(jsonSchema)               # Set the schema of the JSON data\n#     .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n#     .json(inputPath)\n# )\n\n\n# EH_SASL = \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='$ConnectionString' password='Endpoint=sb://\"+eh_account+\".servicebus.windows.net;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=\"+eh_key+\"';\"\n# BOOTSTRAP_SERVERS = eh_account+\".servicebus.windows.net:9093\"\n\n# GROUP_ID = \"$Default\"\n# df = streamingInputDF.selectExpr(\"CAST(time AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\\n#   .writeStream \\\n#   .format(\"kafka\") \\\n#   .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS) \\\n#   .option(\"topic\", \"kafka_spark\") \\\n#   .option(\"kafka.sasl.mechanism\",\"PLAIN\") \\\n#   .option(\"kafka.security.protocol\",\"SASL_SSL\") \\\n#   .option(\"kafka.sasl.jaas.config\", EH_SASL ) \\\n#   .option(\"checkpointLocation\", \"dbfs:/mnt/demo/checkpoint19\") \\\n#   .start() \n\n"],"metadata":{},"outputs":[],"execution_count":11}],"metadata":{"name":"encryption_template","notebookId":1113761770974841},"nbformat":4,"nbformat_minor":0}