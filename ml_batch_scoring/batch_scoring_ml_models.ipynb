{"cells":[{"cell_type":"markdown","source":["# Batch Scoring ML Model with Spark Pandas UDF\n\nAfter you train your ML model, how do you use it to perform batch scoring of a very large dataset?  How would you do this in parallel to minimize scoring time?.\nIf you trained your model with Spark ML then this is not a problem as Spark ML model is designed to score Spark distributed data objects. However, if Spark ML is not what you used due to its limitation and your model happens to be SKlearn or a Tensorflow model or  is in the form of published web service (your own model or a cognitive API) then there's no straight forward way to do this.\n \nIn this post, I'll show two examples of how batch scoring can be applied using the relatively new Pandas UDF function in Spark 2.x:\nBatch scoring from cognitive API (or your own ML model published as API)\nBatch scoring from persisted SKlearn model"],"metadata":{}},{"cell_type":"markdown","source":["## Scoring from persisted sklearn model"],"metadata":{}},{"cell_type":"code","source":["#This is the example to load a sklearn model from pkl file and use it to score mini batches of data from Spark Streaming.\n#But the dataset does not need to be streaming, it can be any Spark dataset\ndf_stream = spark.readStream.format(\"delta\").table(\"events\")\ndf_stream.withWatermark(\"aiv_epoch_start\", \"10 minutes\").registerTempTable(\"amazon_msess_events\")\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType,window\nimport datetime\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.externals import joblib\nimport pandas.errors\n\njdbcHostname = 'DBhost'\njdbcUsername =''\njdbcPassword = ''\ntable = ''\n\njdbcDatabase = \"DBname\"\njdbcPort = 1433\njdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\nprint(jdbcUrl)\nconnectionProperties = {\n  \"user\" : jdbcUsername,\n  \"password\" : jdbcPassword,\n  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n}\n\n\n\nreq_eval = spark.sql(\"select ng_start_time, ng_stop_time, cast(ng_stop_time as long) as start_time,cast(ng_stop_time as long) as stop_time, (cast(ng_stop_time as long) - cast(ng_start_time as long)) as duration, total_count, manifest_requests, avg_request_time,avg_tdwait_time,max_request,max_tdwait,  max_mbps, min_mbps,avg_mbps,avg_rtt,bytes,total_2xx,total_3xx,total_4xx,total_5xx,td_lte_1s_count,tc_lte_1s_count,\ttd_gt_1s_lte_2s_count,\ttc_gt_1s_lte_2s_count,\ttd_gt_2s_lte_4s_count,\ttc_gt_2s_lte_4s_count,\ttd_gt_4s_lte_6s_count,\ttc_gt_4s_lte_6s_count,\ttd_gt_6s_lte_8s_count,\ttc_gt_6s_lte_8s_count,\ttd_gt_8s_lte_10s_count,\ttc_gt_8s_lte_10s_count,\ttd_gt_10s_lte_30s_count,\ttc_gt_10s_lte_30s_count,\ttd_gt_30s_lte_60s_count,\ttc_gt_30s_lte_60s_count,\ttd_gt_60s_count\ttc_gt_60s_count,asn, cc, state,cast(max_err as string) as max_err, (case when aiv_num_rebuffers >0 then 1 else 0 end) as aiv_num_rebuffers  from amazon_msess_events\")\n\n\nschema = StructType([\n    StructField(\"ng_start_time\", TimestampType()),\n    StructField(\"ng_stop_time\", TimestampType()),\n    StructField(\"cc\", StringType()),\n    StructField(\"state\", StringType()),\n    StructField(\"asn\", DoubleType()),\n\n    StructField(\"predicted_buffering\", DoubleType()),\n    StructField(\"actual_buffering\", DoubleType()),\n\n])\n\n@pandas_udf(schema, functionType=PandasUDFType.GROUPED_MAP)\n\n\ndef predict_buffering(panda_buffer):\n  from sklearn.externals import joblib\n  model = joblib.load('/dbfs/mnt/regr.joblib') \n\n  #if datasize is not long enough, return default result\n  #score threshold whether to score as positive\n  threshold = 0.45\n  states=['SC','AZ','LA','MN','NJ','DC','OR','unknown','VA','RI','KY','WY','NH','MI','NV','WI','ID','CA','CT','NE','MT','NC','VT','MD','DE','MO','IL','ME','WA','ND','MS','AL','IN','OH','TN','IA','NM','PA','SD','NY','TX','WV','GA','MA','KS','CO','FL','AK','AR','OK','UT','HI']\n  countries =['cr','pr','us','vi','cl','il','ro','jp','ag','vn','pl','vg','za','sk','mu','pt','ke','ni','sg','ae','iq','hk','be','qa','bz','gb','me','ec','sa','co','tr','de','is','tt','lu','br', 'im', 'gt', 'bb', 'jo', 'es', 'hr', 'eu', 'dj', 'kr', 'it', 'uy', 'af', 'pe', 'vc', 'ar', 'sv', 'jm', 'ph', 'nl', 'bo', 'gp', 'hn', 'hu', 'ca', 'al', 'bm', 've', 'gu','ee','nz','si','gr','aw','ru','mt','th','cw','ch','ma','gh','do','lt','ht','pa','no','bg','cy','at','cz','ua','dm','mx','bs','ai','ky','fr','se','ie','dk','gd','id','bh','gg','gy','fi']\n\n  errors =[412, 206, 500, 504, 502, 503, 400, 403, 404, 591, 408, 200, 260, 499]\n  ori_states = panda_buffer['state'].values.tolist()\n  ori_cc=panda_buffer['cc'].values.tolist()\n  panda_buffer['ng_start_time'] = pd.to_datetime(panda_buffer['start_time'], unit='ms', utc=True)\n  panda_buffer['ng_stop_time'] = pd.to_datetime(panda_buffer['stop_time'],unit='ms', utc=True)\n  start_time = panda_buffer['ng_start_time'].values.tolist()\n  stop_time = panda_buffer['ng_stop_time'].values.tolist()\n  del panda_buffer['ng_start_time']\n  del panda_buffer['ng_stop_time']\n  del panda_buffer['start_time']\n  del panda_buffer['stop_time']\n#   del panda_buffer['window']\n  panda_buffer['state']= panda_buffer['state'].astype('category',categories=states)\n  temp = pd.get_dummies(panda_buffer['state'], prefix='state')\n  panda_buffer = pd.concat([panda_buffer, temp], axis = 1)\n  del panda_buffer['state'], temp\n\n  panda_buffer['cc']= panda_buffer['cc'].astype('category',categories=countries)\n  temp = pd.get_dummies(panda_buffer['cc'], prefix='cc')\n  panda_buffer = pd.concat([panda_buffer, temp], axis = 1)\n  del panda_buffer['cc'], temp\n\n  panda_buffer['max_err']= panda_buffer['max_err'].astype('category',categories=errors)\n  temp = pd.get_dummies(panda_buffer['max_err'], prefix='max_err')\n  panda_buffer = pd.concat([panda_buffer, temp], axis = 1)\n  del panda_buffer['max_err'], temp\n\n  # panda_buffer['asn']= panda_buffer['asn'].astype('category',categories=asns)\n  # temp = pd.get_dummies(panda_buffer['asn'], prefix='asn')\n  # panda_buffer = pd.concat([panda_buffer, temp], axis = 1)\n  # del panda_buffer['asn'], temp\n  asn =panda_buffer['asn'].values.tolist()\n  y = panda_buffer['aiv_num_rebuffers'].values.tolist()\n  X_dataset=  panda_buffer.copy()\n  del X_dataset['aiv_num_rebuffers']\n  X = X_dataset.values\n\n  y_pred= model.predict(X).tolist()\n  def score(y_pred, threshold=0.5):\n    out = []\n    for item in y_pred:\n      if item[1] >=threshold:\n        out.append(1)\n      else:\n        out.append(0)\n    return out\n  out_pred = model.predict_proba(X).tolist()\n  y_pred = score(out_pred, threshold)\n\n\n  return pd.DataFrame({'ng_start_time': start_time,'ng_stop_time': stop_time, 'cc':ori_cc,'state':ori_states,'asn':asn,  'predicted_buffering': y_pred, 'actual_buffering': y})\n\n\n\n\n\n\n\nagg= req_eval.groupby(window(\"ng_start_time\", \"5 minutes\", \"10 seconds\"))\noutput =agg.apply(predict_buffering)\n\n#here you can set the mode = \"overwrite\" (inside JDBC) if you only want to see latest data in output table. Otherwise can you set the mode ='append', then in the query of client tool, you need to select latest record sort by start_time, end_time\n#side note: if you want to ouput a regular Dataframe, not stream dataframe to SQL table, use syntax: yourDF.write.jdbc(url=jdbcUrl, table=table, mode=\"append\", properties=connectionProperties)\n#IMPORTANT: Please make sure you create output table in advance in SQL Server. There's a bug for non-numeric column to be included as colum index if you let the driver create table automatically.\noutput.registerTempTable(\"prediction_out\")\n\n  "],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Batch Scoring from ML rest API"],"metadata":{}},{"cell_type":"code","source":["\n#This Example use the Azure's Anomaly Finder API to score against Spark data in a batch fashion.\n#By doing this, we can score records in parallel for supposed to be real time API.\n\n#This function has input of a spark data frame which include mulitple variables aligned in the same timeline to test for anomalies simultaneously\n# @columns: This is the list of numeric column to detect anomaly\n# @cat_columns: this the list of categorical columns to include for query purpose\n# @Groupby_cols: the list of columns for grouping by\ndef anomaly_df_finder(df,columns, cat_columns,groupby_cols,max_ratio=0.25,sens=95, rare_as_exception =False,higher_value_better_cols=None ):\n  from pyspark.sql.types import StructType,StructField,StringType,BooleanType, TimestampType,DoubleType\n  from pyspark.sql.functions import pandas_udf, PandasUDFType\n  import datetime\n  from pyspark.sql.functions import window\n\n  import pandas as pd\n  import json\n  import requests\n  import numpy as np\n  import time\n\n  schema = StructType([\n      StructField(\"timestamp\", TimestampType()),\n      StructField(\"col_anomaly_count\", DoubleType()),\n\n  ])\n  for cat_column in cat_columns:\n    schema.add(cat_column,StringType() )\n\n  for column in columns:\n    schema.add(column+'_IsAnomaly_Pos',BooleanType() )\n    schema.add(column+'_IsAnomaly_Neg',BooleanType() )\n    schema.add(column+'_is_anomaly',BooleanType() )\n    schema.add(column+'_upper_value',DoubleType() )\n    schema.add(column+'_lower_value',DoubleType() )\n    schema.add(column+'_expected_value',DoubleType() )\n    schema.add(column+'_value',DoubleType() )\n\n  @pandas_udf(schema, functionType=PandasUDFType.GROUPED_MAP)\n\n\n  def detect_anomaly(df):\n    MaxAnomalyRatio=max_ratio\n    Sensitivity=sens\n\n  #   columns = ['avg_rtt', 'avg_tdwait']\n    output_dict = {'timestamp': df['Timestamp']}\n    output_dict['col_anomaly_count']=0\n    for cat_column in cat_columns:\n      output_dict[cat_column]= df[cat_column]\n\n    #if datasize is not long enough, return default result\n    if df.shape[0] <12:\n      for column in columns:\n        output_dict[column+'_IsAnomaly_Pos']=rare_as_exception\n        output_dict[column+'_IsAnomaly_Neg']=False\n        output_dict[column+'_is_anomaly']=rare_as_exception\n        output_dict[column+'_upper_value']=-999\n        output_dict[column+'_lower_value']=-999\n        output_dict[column+'_value']=df[column]\n        output_dict[column+'_expected_value']=df[column]\n      output_dict['col_anomaly_count']=output_dict['col_anomaly_count']+np.array(output_dict[column+'_IsAnomaly_Pos'])\n\n      return pd.DataFrame(data=output_dict)\n\n\n\n\n    endpoint = 'https://westus2.api.cognitive.microsoft.com/anomalyfinder/v2.0/timeseries/entire/detect'\n    subscription_key = '' #Key for version 2\n\n    def detect(endpoint, subscription_key, request_data):\n      headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key': subscription_key}\n      response = requests.post(endpoint, data=json.dumps(request_data), headers=headers)\n      #Dealing with threshold exceeding exception, retry util we can call the api\n      while response.status_code == 429:\n        time.sleep(1)\n        response = requests.post(endpoint, data=json.dumps(request_data), headers=headers)\n\n      if response.status_code == 200:\n          return json.loads(response.content.decode(\"utf-8\"))\n\n      else:\n  #         print(response.status_code)\n          raise Exception(str(response.status_code)+\":\" +response.text + json.dumps(request_data))\n\n    #Loop for each column in \n#     df.sort_values(by= 'Timestamp', inplace=True)\n    \n    for column in columns:\n      df_out = df[['Timestamp', column]].copy()\n      df_out[\"Value\"] = df_out[column]\n      del df_out[column]\n      df_out.Timestamp = pd.to_datetime(df.Timestamp, unit ='ms',utc =True)\n      json_data = df_out.to_json(orient='records',date_format ='iso',date_unit ='s')\n      json_loaded = json.loads(json_data)\n      json_loaded = {\"Granularity\":\"minutely\", \"CustomInterval\":5,\"MaxAnomalyRatio\": MaxAnomalyRatio, \"Sensitivity\": Sensitivity, \"Series\":json_loaded }\n    #   json_loaded = {\"Period\": None, \"Points\":json_loaded }\n\n      try:\n        result = detect(endpoint, subscription_key, json_loaded)\n      except Exception as e:\n        output_dict[column+'_IsAnomaly_Pos']=rare_as_exception\n        output_dict[column+'_IsAnomaly_Neg']=False\n        output_dict[column+'_is_anomaly']=rare_as_exception\n        output_dict[column+'_upper_value']=-999\n        output_dict[column+'_lower_value']=-999\n        output_dict[column+'_value']=df[column]\n        output_dict[column+'_expected_value']=df[column]\n        continue\n\n\n      output_dict[column+'_IsAnomaly_Pos']=result['IsPositiveAnomaly']\n      output_dict[column+'_IsAnomaly_Neg']=result['IsNegativeAnomaly']\n      output_dict[column+'_is_anomaly']=result['IsAnomaly']\n      if higher_value_better_cols and column in higher_value_better_cols:\n        output_dict['col_anomaly_count']=output_dict['col_anomaly_count']+np.array(result['IsNegativeAnomaly'])\n      else:\n        output_dict['col_anomaly_count']=output_dict['col_anomaly_count']+np.array(result['IsPositiveAnomaly'])\n\n#       output_dict['col_anomaly_count']=output_dict['col_anomaly_count']+np.array(result['IsAnomaly'])\n\n      output_dict[column+'_upper_value']=result['UpperMargins']\n      output_dict[column+'_lower_value']=result['LowerMargins']\n      output_dict[column+'_value']=df[column]\n      output_dict[column+'_expected_value']=result['ExpectedValues']\n#       output_dict[column+'_upper_value'] = np.array(result['ExpectedValues'])+(100-Sensitivity)*np.array(result['UpperMargins'])\n#       output_dict[column+'_lower_value'] = np.array(result['ExpectedValues'])-(100-Sensitivity)*np.array(result['LowerMargins']) \n      output_dict[column+'_upper_value'] = np.array(result['UpperMargins'])\n      output_dict[column+'_lower_value'] = np.array(result['LowerMargins'])  \n\n    output = pd.DataFrame(data=output_dict)\n\n    return output\n  agg=df.groupby(groupby_cols)\n  df_output =agg.apply(detect_anomaly)\n\n  return df_output"],"metadata":{},"outputs":[],"execution_count":5}],"metadata":{"name":"batch_scoring_ml_models","notebookId":3972720607427244},"nbformat":4,"nbformat_minor":0}